---
layout: post
title: SVD分解初步
date: 2022-09-27
tags: 大数据分析
---

### 数据降维问题

大数据的变量（维数）较多，产生了巨大的分析与计算复杂度。而这些变量之间是存在关联的。

人们希望通过克服相关性、重叠性，用较少的变量来代替原来较多的变量，这就是一种降维的思想。

考虑以下矩阵：
$$
\begin{bmatrix}
        a\\\\ 
        b\\\\
        c\\\\
\end{bmatrix}
=
\begin{bmatrix}
        1 & 2 & 1\\\\ 
        -2 & -3 & 1\\\\
        3 & 5 & 0\\\\
\end{bmatrix}
$$
它有三个基：[1 0 0] [0 1 0] [0 0 1]，

现在我们定义新的基：[1 2 1] [-2 -3 1]，

此时a、b、c可用新的坐标表示：a[1 0] b[0 1] c[1 -1]。此时我们便用两个变量表示了原来用三个变量表示的对象，实现了降维的效果。

### 奇异值分解SVD

#### 特征值与特征向量

设A为n阶矩阵，如果数$\lambda$和n为非零列向量$x$使关系式
$$
Ax=\lambda x
$$
成立，则数$\lambda$为方阵A的特征值，$x$为对应于数$\lambda$的一个特征向量。

#### SVD

对于秩为r的矩阵$A_{m\times n}$，必存在m阶正交矩阵$U$、n阶正交矩阵$V$、$m\times n$矩阵$\sum$，使得 
$$
A=U\sum V^T
$$
，其中
$$
\sum = \begin{bmatrix}
D_{r\times r}&O\\
O &O
\end{bmatrix}
$$
，$D_{r\times r}$为对角矩阵，$D_{ii}=\sqrt{\lambda_i}$

#### 近似SVD

我们不一定需要全部的奇异值，可以用前几个最大的奇异值来近似。
$$
A_{m\times n}\approx U_{m\times r}\Sigma_{r\times r}V_{r\times n}^T
$$

- A：$m\times n$矩阵（m个文本、n个名词）
- U：$m\times r$矩阵（m个文本、r个概念）
- $\Sigma$：$r\times r$对角矩阵（每个概念的强度）
- V：$n\times r$矩阵（n个名词、r个概念

其中有$r\le R(A)$